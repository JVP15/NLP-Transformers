{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorda\\.conda\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# required to download at least once\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertForTokenClassification,\n",
    "    BertConfig, \n",
    "    AutoTokenizer, \n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import evaluate\n",
    "\n",
    "SEQEVAL = evaluate.load('seqeval')\n",
    "\n",
    "TRAIN_SAMPLES = 10000\n",
    "EVAL_SAMPLES = 1000\n",
    "SEED = 42\n",
    "\n",
    "LABELS = ['middle-of-token', 'end-of-token']\n",
    "\n",
    "MAX_SEQ_LEN = 512 # this includes the EOS token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text using nltk's word tokenizer (with some modification for quotes), \n",
    "    except for whitespace, where each single space, tab, etc. is treated as it's own token\n",
    "    \"\"\"\n",
    "\n",
    "    # split on any whitespace character, also keeping the whitespace characters as tokens \n",
    "    tokens = re.split(r'(\\s+)', text)\n",
    "\n",
    "    # at this point though, some whitespace tokens contain multiple characters e.g. '  ', but I only want 1 char/whitespace token like ' ', ' '\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isspace():\n",
    "            for char in token:\n",
    "                new_tokens.append(char)\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "    tokens = new_tokens\n",
    "\n",
    "    # now tokenize each non-whitespace token using nltk's word tokenizer\n",
    "    final_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isspace():\n",
    "            final_tokens.append(token)\n",
    "        else:\n",
    "            final_tokens.extend(nltk.word_tokenize(token))\n",
    "\n",
    "    # nltk also has an annoying 'feature' where it converts double quotes to either `` or '' in a destructive manner, but I can't have that\n",
    "    # so I need to go through all of the tokens, check if it *should* be double quotes (and isn't) and update the tokens if that is the case\n",
    "    for i, token in enumerate(final_tokens):\n",
    "        if token in ['``', \"''\"] and (i == 0 or final_tokens[i-1] != '\"') and (i == len(final_tokens) - 1 or final_tokens[i+1] != '\"'):\n",
    "            final_tokens[i] = '\"'\n",
    "\n",
    "    return final_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_text(text: str) -> list[str]:\n",
    "    \"\"\"This is like function above, but it uses a different thing under the hood b/c the default tokenizer has a lot of problems for this application\"\"\"\n",
    "    spans = nltk.tokenize.NLTKWordTokenizer().span_tokenize(text)\n",
    "\n",
    "    labels = np.ones((len(text),), dtype=np.long) * -1 # unlabelled stuff will be -1 for now, will set to 1 later\n",
    "\n",
    "    for span in spans:\n",
    "        labels[span[0]:span[1] - 1] = 0 # set everything within the span to 0 \n",
    "        labels[span[1] - 1] = 1 # then set the end of the span properly (the upper range of the span is exclusive, hence -1)\n",
    "\n",
    "    labels[labels == -1] = 1 # anything that isn't in a span is probably whitespace, so those can all be labelled \n",
    "\n",
    "    return labels\n",
    "\n",
    "def create_bert_model(vocab_size=256, hidden_size=384, num_hidden_layers=6, num_attention_heads=12, intermediate_size=1536):    \n",
    "    config = BertConfig(\n",
    "        vocab_size=vocab_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        num_attention_heads=num_attention_heads,\n",
    "        intermediate_size=intermediate_size,\n",
    "        pad_token_id=26, # in ascii this is the SUB character, which I will use as padding\n",
    "        num_labels=2,  # the only two choices are not end of nltk token and end of nltk token\n",
    "    )\n",
    "\n",
    "    model = BertForTokenClassification(config)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [LABELS[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [LABELS[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = SEQEVAL.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_tokenize = AutoTokenizer.from_pretrained('google/byt5-small', clean_up_tokenization_spaces=False)\n",
    "byte_tokenize.pad_token_id = 26\n",
    "byte_tokenize.eos_token_id = 3  # in ascii this is the ETX character, which I will use as the end-of-sequence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_textbooks = load_dataset('nampdn-ai/tiny-textbooks')\n",
    "\n",
    "tiny_textbooks = tiny_textbooks.shuffle(seed=42)\n",
    "tiny_textbooks['train'] = tiny_textbooks['train'].select(range(TRAIN_SAMPLES))\n",
    "tiny_textbooks['test'] = tiny_textbooks['test'].select(range(EVAL_SAMPLES))\n",
    "\n",
    "# since we label a word/token by it's last character, we have to be careful how we tokenize the text so as not to loose \n",
    "# a label if the word gets truncated halfway through, which is why we truncate all texts ourselves to MAX_SEQ_LEN-1 (the -1 accounts for the EOS token)\n",
    "tiny_textbooks = tiny_textbooks.map(\n",
    "    lambda examples: {'text': examples['text'][:MAX_SEQ_LEN-1]} \n",
    ").map(\n",
    "    lambda examples: {'labels': label_text(examples['text'])}\n",
    ").map(\n",
    "    lambda examples: byte_tokenize(examples['text'], truncation=True, max_length=MAX_SEQ_LEN, padding='do_not_pad'), batched=True, # the collator will take care of padding\n",
    "    remove_columns=['text', 'source', 's', 'len', 'idx', 'textbook']\n",
    ")\n",
    "\n",
    "collator = DataCollatorForTokenClassification(tokenizer=byte_tokenize, padding=True, max_length=MAX_SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "model = create_bert_model()\n",
    "\n",
    "print(model.num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorda\\.conda\\envs\\nlp\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=2,\n",
    "    seed=SEED,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    bf16=True,\n",
    "    lr_scheduler_type='cosine',\n",
    "    report_to='wandb',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tiny_textbooks['train'],\n",
    "    eval_dataset=tiny_textbooks['test'],\n",
    "    tokenizer=byte_tokenize,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjvp15\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jorda\\Documents\\Python\\NLP-LLMs\\wandb\\run-20240913_115446-3jm23mv2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jvp15/huggingface/runs/3jm23mv2' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/jvp15/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jvp15/huggingface' target=\"_blank\">https://wandb.ai/jvp15/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jvp15/huggingface/runs/3jm23mv2' target=\"_blank\">https://wandb.ai/jvp15/huggingface/runs/3jm23mv2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/158 [00:00<?, ?it/s]c:\\Users\\jorda\\.conda\\envs\\nlp\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jorda\\.conda\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 79/158 [00:23<00:21,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5384, 'grad_norm': 0.25033038854599, 'learning_rate': 5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorda\\.conda\\envs\\nlp\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: middle-of-token seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\jorda\\.conda\\envs\\nlp\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: end-of-token seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "                                                \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 79/158 [00:32<00:21,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5161757469177246, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 0.7711452369915983, 'eval_runtime': 8.7463, 'eval_samples_per_second': 114.335, 'eval_steps_per_second': 1.829, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorda\\.conda\\envs\\nlp\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 158/158 [01:05<00:00,  3.61it/s]c:\\Users\\jorda\\.conda\\envs\\nlp\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2888: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5168, 'grad_norm': 0.2454090267419815, 'learning_rate': 0.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jorda\\.conda\\envs\\nlp\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: middle-of-token seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\jorda\\.conda\\envs\\nlp\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: end-of-token seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 158/158 [01:19<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5138279795646667, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 0.7714181370158784, 'eval_runtime': 13.2682, 'eval_samples_per_second': 75.368, 'eval_steps_per_second': 1.206, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 158/158 [01:19<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 80.3034, 'train_samples_per_second': 249.056, 'train_steps_per_second': 1.968, 'train_loss': 0.52763685395446, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=158, training_loss=0.52763685395446, metrics={'train_runtime': 80.3034, 'train_samples_per_second': 249.056, 'train_steps_per_second': 1.968, 'total_flos': 654232903680000.0, 'train_loss': 0.52763685395446, 'epoch': 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, text):\n",
    "    tokenized = byte_tokenize(text, truncation=True, max_length=MAX_SEQ_LEN, return_tensors=\"pt\").to('cuda')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    characters = byte_tokenize.convert_ids_to_tokens(tokenized.input_ids[0])\n",
    "    \n",
    "    result = []\n",
    "    token = ''\n",
    "    for character, pred in zip(characters, predictions[0]):\n",
    "        token += character\n",
    "        if pred == 1:  # End of token\n",
    "            result.append(token)\n",
    "            token = ''\n",
    "            \n",
    "    print(predictions)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0\n",
      " 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1\n",
      " 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0\n",
      " 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0\n",
      " 1 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1\n",
      " 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0\n",
      " 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 0\n",
      " 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1]\n",
      "['Following', ' ', 'up', ' ', 'on', ' ', 'the', ' ', 'post', ' ', 'Kirkendall', ' ', 'wrote', ' ', 'last', ' ', 'night', ' ', 'about', ' ', 'T.J.', \"'s\", ' ', 'interview', ' ', 'with', ' ', 'ESPN', ' ', '950', ' ', 'in', ' ', 'Philadelphia', ',', ' ', 'we', ' ', 'now', ' ', 'have', ' ', 'the', ' ', 'audio', '.', ' ', 'One', ' ', 'of', ' ', 'the', ' ', 'things', ' ', 'that', ' ', 'stood', ' ', 'out', ' ', 'to', ' ', 'me', ' ', 'was', ' ', 'Houshmandzadeh', \"'s\", ' ', 'inclination', ' ', 'about', ' ', 'his', ' ', 'performance', ' ', 'against', ' ', 'the', ' ', 'Eagles', ' ', 'during', ' ', 'the', ' ', 'infamous', ' ', 'tie', ' ', 'game', ' ', 'could', ' ', 'be', ' ', 'inferred', ' ', 'as', ' ', 'a', ' ', '\"', 'job', ' ', 'interview', '.', '\"', '.', ' ', 'In', ' ', 'the', ' ', 'words', ' ', 'of', ' ', 'Goose', ' ', 'from', ' ', 'Top', ' ', 'Gun', ',', ' ', '\"', 'tag', ' ', 'him', ' ', 'now', ' ', 'or', ' ', 'lose', ' ', 'him', ' ', 'forever', '.', '\"', '.', ' ', '[', 'Editor', \"'s\", ' ', 'update', ':', ' ', 'Below', ' ', 'is', ' ', 'an', ' ', 'integrated', ' ', 'player', ' ', 'for', ' ', 'the', ' ', 'above', ' ', 'interview', ' ', 'in', ' ', 'Philadelphia', ',', ' ', 'thanks', ' ', 'to', ' ', 'Jason', ' ', 'at', ' ', 'Bleeding', ' ', 'Green', ' ', 'Nation', ']', '.', ' ', 'Never', ' ', 'miss', ' ', 'Bengals', ' ', 'news', '!']\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "         1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "         0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "         1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "         0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "         0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "         1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 1, 0, 0, 1, 0]], device='cuda:0')\n",
      "['Following ', 'up ', 'on ', 'the ', 'post ', 'Kirkendall ', 'wrote ', 'last ', 'night ', 'about ', 'T.', 'J.', \"'s \", 'interview ', 'with ', 'ESPN ', '950 ', 'in ', 'Philadelphia,', ' ', 'we ', 'now ', 'have ', 'the ', 'audio.', ' ', 'One ', 'of ', 'the ', 'things ', 'that ', 'stood ', 'out ', 'to ', 'me ', 'was ', \"Houshmandzadeh's \", 'inclination ', 'about ', 'his ', 'performance ', 'against ', 'the ', 'Eagles ', 'during ', 'the ', 'infamous ', 'tie ', 'game ', 'could ', 'be ', 'inferred ', 'as ', 'a ', '\"', 'job ', 'interview.', '\"', '.', ' ', 'In ', 'the ', 'words ', 'of ', 'Goose ', 'from ', 'Top ', 'Gun,', ' ', '\"', 'tag ', 'him ', 'now ', 'or ', 'lose ', 'him ', 'forever.', '\"', '.', ' ', '[', \"Editor's \", 'update:', ' ', 'Below ', 'is ', 'an ', 'integrated ', 'play', 'er ', 'for ', 'the ', 'above ', 'interview ', 'in ', 'Philadelphia,', ' ', 'thanks ', 'to ', 'Jason ', 'at ', 'Bleeding ', 'Green ', 'Nat']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"This is a test sentence.  It's got some punctuation and whitespace.\"\n",
    "\n",
    "test_sentence = \"\"\"Following up on the post Kirkendall wrote last night about T.J.'s interview with ESPN 950 in Philadelphia, we now have the audio. One of the things that stood out to me was Houshmandzadeh's inclination about his performance against the Eagles during the infamous tie game could be inferred as a \"job interview.\". In the words of Goose from Top Gun, \"tag him now or lose him forever.\". [Editor's update: Below is an integrated player for the above interview in Philadelphia, thanks to Jason at Bleeding Green Nation]. Never miss Bengals news!\"\"\"\n",
    "print(label_text(test_sentence))\n",
    "print(nltk_tokenize(test_sentence))\n",
    "print(inference(trainer.model, test_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
